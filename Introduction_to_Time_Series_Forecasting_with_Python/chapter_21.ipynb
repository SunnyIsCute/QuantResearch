{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 21 A Gentle Introduction to the Box-Jenkins Method\n",
        "\n",
        "#### Autoregressive Integrated Moving Average Model\n",
        "\n",
        "An AutoRegressive Integrated Moving Average (ARIMA) model is a class of statistical model for analyzing and forecasting time series data:\n",
        "- **AR**: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
        "- **I**: Integrated. The use of diﬀerencing of raw observations (i.e. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
        "- **MA**: Moving Average. A model that uses the dependency between an observation and residual errors from a moving average model applied to lagged observations.\n",
        "\n",
        "Each of these components are explicitly specified in the model as a parameter. A standard notation is used of **ARIMA(p,d,q)** where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used. The parameters of the ARIMA model\n",
        "are defined as follows:\n",
        "- **p**: The number of lag observations included in the model, also called the lag order.\n",
        "- **d**: The number of times that the raw observations are diﬀerenced, also called the degree of diﬀerencing.\n",
        "- **q**: The size of the moving average window, also called the order of moving average.\n",
        "\n",
        "#### Box-Jenkins Method\n",
        "\n",
        "The **Box-Jenkins method** was proposed by George Box and Gwilym Jenkins in their seminal 1970 textbook Time Series Analysis: Forecasting and Control1. The approach starts with the assumption that the process that generated the time series can be approximated using an ARMA model if it is stationary or an ARIMA model if it is non-stationary. The 2016 5th edition of the\n",
        "textbook (Part Two, page 177) refers to the process as a stochastic model building and that it is an iterative approach that consists of the following 3 steps:\n",
        "1. **Identification**. Use the data and all related information to help select a sub-class of model that may best summarize the data.\n",
        "2. **Estimation**. Use the data to train the parameters of the model (i.e. the coeﬃcients).\n",
        "3. **Diagnostic Checking***. Evaluate the fitted model in the context of the available data and check for areas where the model may be improved.\n",
        "\n",
        "It is an iterative process, so that as new information is gained during diagnostics, you can circle back to step 1 and incorporate that into new model classes. Let’s take a look at these steps in more detail.\n",
        "\n",
        "#### Identification\n",
        "\n",
        "The identification step is further broken down into: *Assess whether the time series is stationary, and if not, how many diﬀerences are required to make it stationary*. Identify the parameters of an ARMA model for the data.\n",
        "\n",
        "- (1) Differencing: Below are some tips during identification:\n",
        "  - **Unit Rot Tests**: Use unit root statistical tests on the time series to determine whether or not it is stationary. Repeat after each round of differencing.\n",
        "  - **Avoid over differencing**: Differencing the time series more than is required can result in the addition of extra serial correlation and additional complexity.\n",
        "- (2) Configuring AR and MA: Two diagnostic plots can be used to help choose the p and q parameters of the ARMA or ARIMA. They are:\n",
        "  - **Autocorrelation Function (ACF)**: The plot summarizes the correlation of an observation with lag values. The x-axis shows the lag and y-axis shows the correlation coefficient between -1 and 1 for negative and positive correltion.\n",
        "  - **Partial Autocorrelation Function (PACF)**: The plot summarizes the correlations for an observation with lag values that is not accounted by prior lagged observations.\n",
        "  - **Difference between ACF and PACF**: ACF(2) = raw correlation between $X_t$ and $X_{t-1}$. PACF(2) = correlation between $X_t$ and $X_{t-2}$ after removing the influcence of $X_{t-1}$.\n",
        "    - ACF = total correlation (direct + indirect) --> identify MA terms in ARIMA models.\n",
        "    - PACF = direct correlation (after removing shorter-lag effects --> identify AR terms in ARIMA models.\n",
        "  - Both plots are drawn as bar charts showing the 95% and 99% confidence intervals as horizontal lines.\n",
        "  - Some useful patterns you may observe on these plots are:\n",
        "    - The model is AR if the ACF trails oﬀ after a lag and has a hard cut-oﬀ in the PACF after a lag. This lag is taken as the value for p.\n",
        "    - The model is MA if the PACF trails oﬀ after a lag and has a hard cut-oﬀ in the ACF after the lag. This lag value is taken as the value for q.\n",
        "    - The model is a mix of AR and MA if both the ACF and PACF trail oﬀ.\n",
        "\n",
        "#### Estimation\n",
        "\n",
        "Estimation involves using numerical methods to minimize a loss or error term.\n",
        "\n",
        "#### Diagnostic Checking\n",
        "\n",
        "The idea of diagnostic checking is to look for evidence that the model is not a good fit for the data. Two useful areas to investigate diagnostics are:\n",
        "- **Overfitting**:\n",
        "The first check is to check whether the model overfits the data. Generally, this means that the model is more complex than it needs to be and captures random noise in the training data. This is a problem for time series forecasting because it negatively impacts the ability of the model\n",
        "to generalize, resulting in poor forecast performance on out-of-sample data. Careful attention must be paid to both *in-sample and out-of-sample performance* and this requires the careful design of a robust test harness for evaluating models.\n",
        "- **Residual Errors**:\n",
        "  - Forecast residuals provide a great opportunity for diagnostics. A review of the distribution of errors can help tease out bias in the model. The errors from an ideal model would resemble *white noise, that is a Gaussian distribution with a mean of zero and a symmetrical variance*. For this, you may use *density plots, histograms, and Q-Q plots* that compare the distribution of errors to the expected distribution. A non-Gaussian distribution may suggest an opportunity for data pre-processing. A skew in the distribution or a non-zero mean may suggest a bias in forecasts that may be correct.\n",
        "  - Additionally, an ideal model would leave *no temporal structure* in the time series of forecast residuals. These can be checked by *creating ACF and PACF plots of the residual error time series*. The presence of serial correlation in the residual errors suggests further opportunity for\n",
        "using this information in the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "RF87RiWpMBFq"
      }
    }
  ]
}