{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 8 Feature Selection For Machine Learning\n",
        "\n",
        "#### 1. Univariate Selection\n",
        "\n",
        "Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the `SelectKBest` class that can be used with a suite of diﬀerent statistical tests to select a specific number of features. The example below uses the chi-squared (chi2) statistical test for non-neg"
      ],
      "metadata": {
        "id": "YhvkEzTzW4zi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olK7mV91W212",
        "outputId": "02dfe14d-faa3-4eb7-fde9-e18844905be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
            "[[148.    0.   33.6  50. ]\n",
            " [ 85.    0.   26.6  31. ]\n",
            " [183.    0.   23.3  32. ]\n",
            " [ 89.   94.   28.1  21. ]\n",
            " [137.  168.   43.1  33. ]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# load data\n",
        "filename = 'data/pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "df = pd.read_csv(filename, names=names)\n",
        "array = df.values\n",
        "X = array[:,:-1]\n",
        "Y = array[:,-1]\n",
        "# feature extraction\n",
        "test = SelectKBest(score_func=chi2, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "# summarize scores\n",
        "np.set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "features = fit.transform(X)\n",
        "# summarize selected features\n",
        "print(features[:5, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Recursive Feature Elimination\n",
        "\n",
        "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
        "You can learn more about the RFE class3 in the scikit-learn documentation. The example below uses `RFE` with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent."
      ],
      "metadata": {
        "id": "hgdTuQVqXVJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction with RFE\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# feature extraction\n",
        "model = LogisticRegression(max_iter=500)\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(f'Num Features: {fit.n_features_}')\n",
        "print(f'Selected Features: {fit.support_}')\n",
        "print(f'Feature Ranking: {fit.ranking_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtIlMZNCXXaQ",
        "outputId": "32790a25-0d57-4274-f0d1-ce4daf2b7ca3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 6 5 1 1 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Principle Component Analysis\n",
        "\n",
        "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal components in the transformed result. In the example below, we use PCA and select 3 principal components."
      ],
      "metadata": {
        "id": "7J2Om6TTXXyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction with PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# feature extraction\n",
        "pca = PCA(n_components=3)\n",
        "fit = pca.fit(X)\n",
        "# summarize components\n",
        "print(f'Explained Variance: {fit.explained_variance_ratio_}')\n",
        "print(fit.components_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtU0lXxPXaOQ",
        "outputId": "8216546c-59fd-49c4-8363-dc774d99ae21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance: [0.889 0.062 0.026]\n",
            "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
            "   5.372e-04 -3.565e-03]\n",
            " [ 2.265e-02  9.722e-01  1.419e-01 -5.786e-02 -9.463e-02  4.697e-02\n",
            "   8.168e-04  1.402e-01]\n",
            " [ 2.246e-02 -1.434e-01  9.225e-01  3.070e-01 -2.098e-02  1.324e-01\n",
            "   6.400e-04  1.255e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explained variance tell you how much of the total variance in your data is captured by each of the 3 principal components.\n",
        "- PC1 (first component): 0.889 → captures 88.9% of the total variance\n",
        "- PC2: 0.062 → captures 6.2%\n",
        "- PC3: 0.026 → captures 2.6%\n",
        "\n",
        "Each row in `fit.components_` corresponds to one principal component,\n",
        "and each column corresponds to an original feature (in your case: preg, plas, pres, skin, test, mass, pedi, age).\n",
        "\n",
        "These are the weights (or loadings) that define how each principal component is constructed from the original variables.\n",
        "- In the first principal component (PC1),\n",
        "the largest weight is for the test feature (≈ 0.9931).\n",
        "→ This means PC1 is primarily driven by the test variable — it contributes most to the variance.\n",
        "- In the second component (PC2), the largest weight is for plas (≈ 0.9722),\n",
        "→ meaning PC2 mainly represents variation in the plas variable.\n",
        "- In the third component (PC3), the largest weight is for pres (≈ 0.9225),\n",
        "→ so PC3 captures variation related to blood pressure."
      ],
      "metadata": {
        "id": "HTATgH9ocMt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Feature Importance\n",
        "\n",
        "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features. In the example below we construct a `ExtraTreesClassifier` classifier for the Pima Indians onset of diabetes dataset."
      ],
      "metadata": {
        "id": "6vhOrIBgXao_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance with Extra Tree Classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# feature extraction\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X, Y)\n",
        "print(model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KHntweqXdAp",
        "outputId": "7552e98a-1b33-45dc-da2c-83e51f043356"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.108 0.238 0.101 0.078 0.077 0.135 0.121 0.142]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that we are given an importance score for each attribute where the larger the score, the more important the attribute. The scores suggest at the importance of plas, age and mass."
      ],
      "metadata": {
        "id": "6t_w6nJqdQgj"
      }
    }
  ]
}